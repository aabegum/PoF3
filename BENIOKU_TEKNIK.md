# PoF3 - Technical Documentation
## Developer & Operations Guide

**Version:** 3.1
**Last Updated:** December 2025
**Python:** 3.12.6
**License:** Enterprise

---

## ğŸ“‹ Table of Contents

1. [System Architecture](#system-architecture)
2. [Installation & Setup](#installation--setup)
3. [Pipeline Stages](#pipeline-stages)
4. [Configuration](#configuration)
5. [Running the Pipeline](#running-the-pipeline)
6. [Output Files](#output-files)
7. [Testing](#testing)
8. [Deployment](#deployment)
9. [Troubleshooting](#troubleshooting)
10. [Development Guidelines](#development-guidelines)
11. [API Reference](#api-reference)
12. [Performance Optimization](#performance-optimization)

---

## ğŸ—ï¸ System Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PoF3 Pipeline                          â”‚
â”‚                                                             â”‚
â”‚  Input Data (Excel)                                         â”‚
â”‚  â”œâ”€â”€ ariza_final.xlsx (Fault events)                        â”‚
â”‚  â””â”€â”€ saglam_final.xlsx (Healthy equipment)                  â”‚
â”‚                    â†“                                         â”‚
â”‚  Stage 01: Data Processing (01_veri_isleme.py)              â”‚
â”‚  â”œâ”€â”€ Turkish date parsing                                   â”‚
â”‚  â”œâ”€â”€ Outlier detection (MAD on log scale)                   â”‚
â”‚  â”œâ”€â”€ Auto-detect DATA_END_DATE                              â”‚
â”‚  â””â”€â”€ Missingness reporting                                  â”‚
â”‚                    â†“                                         â”‚
â”‚  Stage 02: Feature Engineering (02_ozellik_muhendisligi.py) â”‚
â”‚  â”œâ”€â”€ IEEE 1366 chronic flags                                â”‚
â”‚  â”œâ”€â”€ Weighted chronic index                                 â”‚
â”‚  â”œâ”€â”€ Age, MTBF, seasonality features                        â”‚
â”‚  â””â”€â”€ Stress indicators                                      â”‚
â”‚                    â†“                                         â”‚
â”‚  Stage 03: Survival Models (03_sagkalim_modelleri.py)       â”‚
â”‚  â”œâ”€â”€ Cox Proportional Hazards                               â”‚
â”‚  â”œâ”€â”€ Random Survival Forest (RSF)                           â”‚
â”‚  â”œâ”€â”€ XGBoost + CatBoost Ensemble                            â”‚
â”‚  â”œâ”€â”€ Temporal Cross-Validation (3-fold)                     â”‚
â”‚  â”œâ”€â”€ SHAP Feature Importance                                â”‚
â”‚  â””â”€â”€ Survival curve visualization                           â”‚
â”‚                    â†“                                         â”‚
â”‚  Stage 04: Chronic Detection (04_tekrarlayan_ariza.py)      â”‚
â”‚  â”œâ”€â”€ IEEE 1366 rolling window (365 days)                    â”‚
â”‚  â”œâ”€â”€ Poisson probability                                    â”‚
â”‚  â””â”€â”€ Equipment categorization                               â”‚
â”‚                    â†“                                         â”‚
â”‚  Stage 04b: CoF Scoring (04b_risk_scoring.py)               â”‚
â”‚  â”œâ”€â”€ Equipment cost mapping                                 â”‚
â”‚  â”œâ”€â”€ Voltage multipliers                                    â”‚
â”‚  â”œâ”€â”€ Customer impact                                        â”‚
â”‚  â””â”€â”€ MTTR estimation                                        â”‚
â”‚                    â†“                                         â”‚
â”‚  Stage 05: Risk Assessment (05_risk_degerlendirme.py)       â”‚
â”‚  â”œâ”€â”€ Merge PoF Ã— CoF                                        â”‚
â”‚  â”œâ”€â”€ Risk classification (4-tier)                           â”‚
â”‚  â””â”€â”€ Equipment type summaries                               â”‚
â”‚                    â†“                                         â”‚
â”‚  Stage 05: Reporting (05_raporlama_ve_gorsellestirme.py)    â”‚
â”‚  â”œâ”€â”€ Action lists (urgent, CAPEX, maintenance)              â”‚
â”‚  â”œâ”€â”€ Visualizations (PNG charts)                            â”‚
â”‚  â”œâ”€â”€ Excel report                                           â”‚
â”‚  â””â”€â”€ PowerPoint presentation (optional)                     â”‚
â”‚                    â†“                                         â”‚
â”‚  Output Deliverables                                        â”‚
â”‚  â”œâ”€â”€ CSV files (risk_skorlari_pof3.csv, etc.)               â”‚
â”‚  â”œâ”€â”€ PNG visualizations                                     â”‚
â”‚  â”œâ”€â”€ Excel report                                           â”‚
â”‚  â””â”€â”€ PowerPoint deck                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Directory Structure

```
PoF3/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py                 # Centralized configuration
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ 01_veri_isleme.py         # Data processing
â”‚   â”œâ”€â”€ 02_ozellik_muhendisligi.py # Feature engineering
â”‚   â”œâ”€â”€ 03_sagkalim_modelleri.py  # Survival models (MAIN)
â”‚   â”œâ”€â”€ 04_tekrarlayan_ariza.py   # Chronic detection
â”‚   â”œâ”€â”€ 04b_risk_scoring.py       # CoF calculation
â”‚   â”œâ”€â”€ 05_risk_degerlendirme.py  # Risk assessment
â”‚   â””â”€â”€ 05_raporlama_ve_gorsellestirme.py # Reporting
â”œâ”€â”€ orchestrator/
â”‚   â””â”€â”€ run_pipeline.py           # Master runner
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py                 # Logging utilities
â”‚   â”œâ”€â”€ ml_advanced.py            # Advanced ML functions
â”‚   â”œâ”€â”€ survival_plotting.py      # Visualization utilities
â”‚   â”œâ”€â”€ data_processing.py        # Data utilities
â”‚   â”œâ”€â”€ data_validation.py        # Validation utilities
â”‚   â”œâ”€â”€ date_parser.py            # Turkish date parsing
â”‚   â””â”€â”€ translations.py           # Turkish localization
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ girdiler/                 # Input files
â”‚   â”‚   â”œâ”€â”€ ariza_final.xlsx
â”‚   â”‚   â””â”€â”€ saglam_final.xlsx
â”‚   â”œâ”€â”€ ara_ciktilar/             # Intermediate outputs
â”‚   â””â”€â”€ sonuclar/                 # Final results
â”œâ”€â”€ gorseller/                    # Visualizations
â”œâ”€â”€ loglar/                       # Execution logs
â”œâ”€â”€ modeller/                     # Trained models (gitignored)
â”œâ”€â”€ tests/                        # Unit tests (empty - TODO)
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ BENIOKU_MUSTERI.md            # Customer guide
â”œâ”€â”€ BENIOKU_TEKNIK.md             # Technical guide (this file)
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md     # Feature summary
â”œâ”€â”€ QUICK_REFERENCE.md            # Quick start guide
â””â”€â”€ README.md                     # Project overview
```

---
## 4.1 Model Kalibrasyonu ve Temel Oranlar (Base Rates)

PoF3 modeli, "dengesiz veri" (imbalanced data) yanÄ±lgÄ±sÄ±na dÃ¼ÅŸmemek iÃ§in DSOs (DaÄŸÄ±tÄ±m Åirketleri) iÃ§in geÃ§erli olan **gerÃ§ekÃ§i yÄ±llÄ±k arÄ±za oranlarÄ±na** gÃ¶re kalibre edilmiÅŸtir. Model Ã§Ä±ktÄ±larÄ± aÅŸaÄŸÄ±daki endÃ¼stri standartlarÄ± ile uyumlu olacak ÅŸekilde denetlenir:

| VarlÄ±k Tipi | Beklenen YÄ±llÄ±k ArÄ±za OranÄ± | Kalibrasyon Notu |
|:---|:---|:---|    
| **GÃ¼Ã§ Trafosu** | %0.5 â€“ %5.0 | YaÅŸ ve yÃ¼klenme durumuna duyarlÄ± |
| **Kesici (Breaker)** | %3.0 â€“ %8.0 | BakÄ±m geÃ§miÅŸi ve mekanik aÅŸÄ±nma odaklÄ± |
| **AyÄ±rÄ±cÄ± (Switch)** | %5.0 â€“ %12.0 | Ã‡evresel faktÃ¶rler (korozyon/nem) aÄŸÄ±rlÄ±klÄ± |
| **Hatlar (OH/UG)** | %0.5 â€“ %15.0 | Hava durumu ve dÄ±ÅŸ etkenler (kazÄ± vb.) |
| **Sigortalar** | %15.0 â€“ %30.0 | Operasyonel "sigorta atmasÄ±" dahil |
| **Direkler** | %0.1 â€“ %3.0 | Sadece fiziksel/yapÄ±sal bÃ¼tÃ¼nlÃ¼k kaybÄ± |

**Not:** Model, her varlÄ±k tipi iÃ§in ayrÄ± ayrÄ± eÄŸitilmiÅŸ (stratified) ve bu taban oranlara gÃ¶re doÄŸrulanmÄ±ÅŸtÄ±r (walk-forward validation).
## ğŸ”§ Installation & Setup

### Prerequisites

- **Python:** 3.12.6+ (3.10+ should work)
- **OS:** Windows 10/11, Linux, macOS
- **RAM:** 4GB minimum, 8GB recommended
- **Disk:** 2GB for environment + data

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd PoF3
```

### Step 2: Create Virtual Environment

```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/macOS
python3 -m venv .venv
source .venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**Optional Dependencies:**

```bash
# For SHAP feature importance
pip install shap>=0.43.0

# For PowerPoint generation
pip install python-pptx>=0.6.21

# For testing
pip install pytest pytest-cov
```

### Step 4: Verify Installation

```bash
python -c "import pandas, lifelines, xgboost, catboost; print('OK')"
```

### Step 5: Prepare Input Data

Place input files in `data/girdiler/`:
- `ariza_final.xlsx` - Fault events
- `saglam_final.xlsx` - Healthy equipment

**Required Columns:**

**ariza_final.xlsx:**
- `cbs_id` - Equipment ID
- `Ariza_Baslangic_Zamani` - Fault start datetime
- `Ekipman_Tipi` - Equipment type
- `Sure_Saat` - Duration (hours)

**saglam_final.xlsx:**
- `cbs_id` - Equipment ID
- `Ekipman_Tipi` - Equipment type
- Optional: voltage, location, manufacturer

---

## ğŸš€ Running the Pipeline

### Full Pipeline (Recommended)

```bash
python orchestrator/run_pipeline.py
```

**Output:**
- Master log: `loglar/pipeline_master_YYYYMMDD_HHMMSS.log`
- Execution summary in console
- All CSV/PNG outputs in `data/sonuclar/` and `gorseller/`

**Expected Runtime:** 2-5 minutes (depends on data size)

### Individual Stages (Development)

```bash
# Stage 01: Data Processing
python pipeline/01_veri_isleme.py

# Stage 02: Feature Engineering
python pipeline/02_ozellik_muhendisligi.py

# Stage 03: Survival Models (most time-intensive)
python pipeline/03_sagkalim_modelleri.py

# Stage 04: Chronic Detection
python pipeline/04_tekrarlayan_ariza.py

# Stage 04b: CoF Scoring
python pipeline/04b_risk_scoring.py

# Stage 05: Risk Assessment
python pipeline/05_risk_degerlendirme.py

# Stage 05: Reporting
python pipeline/05_raporlama_ve_gorsellestirme.py
```

### Debug Mode

```bash
# Enable verbose logging
export LOG_LEVEL=DEBUG  # Linux/macOS
set LOG_LEVEL=DEBUG     # Windows

python orchestrator/run_pipeline.py
```

---

## âš™ï¸ Configuration

### config/config.py

**Key Sections:**

#### 1. Directory Paths

```python
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
INPUT_DIR = DATA_DIR / "girdiler"
INTERMEDIATE_DIR = DATA_DIR / "ara_ciktilar"
OUTPUT_DIR = DATA_DIR / "sonuclar"
VISUALIZATIONS_DIR = PROJECT_ROOT / "gorseller"
LOG_DIR = PROJECT_ROOT / "loglar"
MODEL_DIR = PROJECT_ROOT / "modeller"
```

#### 2. Analysis Parameters

```python
ANALYSIS_DATE = None  # Auto-detected from DATA_END_DATE
MIN_DATA_SPAN_YEARS = 2.0  # Minimum historical data
MIN_TRAIN_YEARS = 2.0      # Training data before T_ref
```

#### 3. Survival Horizons

```python
SURVIVAL_HORIZONS = [90, 180, 365, 730]  # days
SURVIVAL_HORIZONS_MONTHS = [3, 6, 12, 24]  # labels
```

**To add new horizon:**
```python
SURVIVAL_HORIZONS = [90, 180, 365, 730, 1095]  # Add 36 months
SURVIVAL_HORIZONS_MONTHS = [3, 6, 12, 24, 36]
```

#### 4. Chronic Detection Settings

```python
CHRONIC_THRESHOLD_EVENTS = 3  # IEEE 1366: â‰¥4 events = chronic
CHRONIC_WINDOW_DAYS = 90      # Optimized from 365
CHRONIC_MIN_RATE = 1.5        # failures/year
```

#### 5. ML Settings

```python
USE_ML = True
RANDOM_STATE = 42
RSF_N_ESTIMATORS = 100
RSF_MIN_SAMPLES_SPLIT = 10
RSF_MIN_SAMPLES_LEAF = 5
```

#### 6. Risk Matrix

```python
RISK_MATRIX = {
    'DÃœÅÃœK': {'pof_max': 0.3, 'cof_max': 3.0},
    'ORTA': {'pof_max': 0.6, 'cof_max': 6.0},
    'YÃœKSEK': {'pof_max': 0.8, 'cof_max': 8.0},
    'KRÄ°TÄ°K': {'pof_max': 1.0, 'cof_max': 10.0}
}
```

---

## ğŸ” Pipeline Stages

### Stage 01: Data Processing

**File:** `pipeline/01_veri_isleme.py`

**Purpose:** Load and clean raw fault + healthy equipment data

**Key Functions:**

1. **`load_and_validate_fault_data()`**
   - Loads `ariza_final.xlsx`
   - Validates required columns
   - Parses Turkish dates (mixed formats)
   - Handles duration outliers (MAD on log scale)

2. **`load_and_validate_healthy_data()`**
   - Loads `saglam_final.xlsx`
   - Normalizes equipment IDs
   - Extracts metadata

3. **`create_survival_base()`**
   - Creates target variable: `event` (1=fault, 0=censored)
   - Calculates `duration_days`
   - Merges fault + healthy equipment

**Outputs:**
- `fault_events_clean.csv`
- `healthy_equipment_clean.csv`
- `equipment_master.csv`
- `survival_base.csv`
- `data_range_metadata.csv`

**Performance:** ~45 seconds

---

### Stage 02: Feature Engineering

**File:** `pipeline/02_ozellik_muhendisligi.py`

**Purpose:** Create predictive features

**Key Features:**

1. **IEEE 1366 Chronic Flags**
   ```python
   kronik_flag = (ariza_sayisi_365gun >= 4) & (poisson_p < 0.05)
   ```

2. **Weighted Chronic Index**
   ```python
   chronic_index = w1*rate_30d + w2*rate_90d + w3*rate_365d
   ```

3. **Age Features**
   - `ekipman_yasi_gun` - Equipment age in days
   - `son_ariza_sonrasi_gun` - Days since last fault

4. **MTBF (Mean Time Between Failures)**
   ```python
   MTBF = total_operational_days / fault_count
   ```

5. **Seasonality Features**
   ```python
   month_sin = sin(2Ï€ * month / 12)
   month_cos = cos(2Ï€ * month / 12)
   ```

6. **Stress Indicators**
   - Maintenance frequency
   - Fault severity
   - Equipment type risk profile

**Outputs:**
- `ozellikler_pof3.csv` - Feature matrix

**Performance:** ~3 seconds

---

### Stage 03: Survival Models

**File:** `pipeline/03_sagkalim_modelleri.py`

**Purpose:** Train survival models and generate PoF predictions

**Models:**

#### 1. Cox Proportional Hazards

```python
from lifelines import CoxPHFitter

cox = CoxPHFitter()
cox.fit(df, duration_col='duration_days', event_col='event')

# Predict survival function
survival_func = cox.predict_survival_function(X_new)
pof_12m = 1 - survival_func.loc[365]  # 12-month PoF
```

**Outputs:**
- `cox_sagkalim_3ay_ariza_olasiligi.csv`
- `cox_sagkalim_6ay_ariza_olasiligi.csv`
- `cox_sagkalim_12ay_ariza_olasiligi.csv`
- `cox_sagkalim_24ay_ariza_olasiligi.csv`
- `cox_coefficients.png` - Hazard ratios

#### 2. Random Survival Forest (RSF)

```python
from sksurv.ensemble import RandomSurvivalForest

rsf = RandomSurvivalForest(
    n_estimators=100,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)
rsf.fit(X, y)

# Feature importance
feature_importance = rsf.feature_importances_
```

**Outputs:**
- `rsf_sagkalim_3ay_ariza_olasiligi.csv`
- `rsf_sagkalim_6ay_ariza_olasiligi.csv`
- `rsf_sagkalim_12ay_ariza_olasiligi.csv`
- `rsf_sagkalim_24ay_ariza_olasiligi.csv`
- `rsf_feature_importance.csv`

#### 3. XGBoost + CatBoost Ensemble

```python
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

# Binary classification for each horizon
for horizon in [365, 1095, 1825]:  # 12, 36, 60 months
    y_binary = (duration_days <= horizon) & (event == 1)

    # XGBoost
    xgb = XGBClassifier(n_estimators=100, max_depth=6)
    xgb.fit(X, y_binary)

    # CatBoost
    cb = CatBoostClassifier(iterations=100, depth=6, verbose=0)
    cb.fit(X, y_binary)

    # Ensemble average
    pof = (xgb.predict_proba(X)[:, 1] + cb.predict_proba(X)[:, 1]) / 2
```

**Outputs:**
- `leakage_free_ml_pof.csv` - Ensemble predictions

#### 4. Advanced Features

**Temporal Cross-Validation:**

```python
from utils.ml_advanced import temporal_cross_validation

cv_results = temporal_cross_validation(
    X=X_xgb,
    y=y,
    model_fn=lambda: XGBClassifier(...),
    n_splits=3,
    logger=logger
)
# Output: temporal_cv_scores.csv
```

**SHAP Feature Importance:**

```python
from utils.ml_advanced import compute_shap_importance

shap_df = compute_shap_importance(
    model=xgb_model,
    X=X_xgb,
    max_samples=1000,
    logger=logger
)
# Output: shap_feature_importance.csv
```

**Visualizations:**

```python
from utils.survival_plotting import (
    plot_survival_curves_by_class,
    plot_cox_coefficients,
    plot_feature_importance_comparison
)

plot_survival_curves_by_class(km_by_type, ...)
# Output: survival_curves_by_class.png

plot_cox_coefficients(cox_model, ...)
# Output: cox_coefficients.png

plot_feature_importance_comparison(rsf_importance, shap_importance, ...)
# Output: feature_importance_comparison.png
```

**Performance:** ~95 seconds (includes temporal CV + SHAP)

---

### Stage 04: Chronic Detection

**File:** `pipeline/04_tekrarlayan_ariza.py`

**Purpose:** IEEE 1366 chronic equipment classification

**Logic:**

```python
# Rolling 365-day window
df['ariza_sayisi_365gun'] = df.groupby('cbs_id')['event'].rolling(
    window='365D', on='Ariza_Baslangic_Zamani'
).sum()

# Poisson probability (null hypothesis: normal failure rate)
expected_rate = 1.5  # failures/year
poisson_p = poisson.sf(ariza_sayisi - 1, expected_rate)

# Chronic flag
kronik_flag = (ariza_sayisi >= 4) & (poisson_p < 0.05)

# Risk categories
if ariza_sayisi >= 6:
    kategori = 'KRÄ°TÄ°K'
elif ariza_sayisi >= 4:
    kategori = 'YÃœKSEK'
elif ariza_sayisi >= 3:
    kategori = 'ORTA'
else:
    kategori = 'DÃœÅÃœK'
```

**Outputs:**
- `chronic_equipment_summary.csv` - All equipment with flags
- `chronic_equipment_only.csv` - Filtered chronic only

**Performance:** ~2 seconds

---

### Stage 04b: CoF Scoring

**File:** `pipeline/04b_risk_scoring.py`

**Purpose:** Calculate Consequence of Failure (CoF)

**Formula:**

```python
CoF = equipment_cost Ã— voltage_multiplier Ã— customer_impact Ã— mttr_factor

# Voltage multipliers
voltage_multipliers = {
    'AlÃ§ak Gerilim': 1.0,
    'Orta Gerilim': 1.5,
    'YÃ¼ksek Gerilim': 2.0
}

# Equipment cost (relative scale 1-10)
equipment_costs = {
    'TransformatÃ¶r': 8.0,
    'Kesici': 7.0,
    'AyÄ±rÄ±cÄ±': 5.0,
    'Sigorta Kutusu': 3.0,
    'Kablo': 6.0
}

# Customer impact (normalized)
customer_impact = min(1 + log10(customer_count + 1) / 4, 2.0)

# MTTR factor (hours)
mttr_factor = 1 + (mttr_hours / 24)  # Capped at reasonable values
```

**Outputs:**
- `cof_pof3.csv` - CoF scores per equipment

**Performance:** ~1 second

---

### Stage 05: Risk Assessment

**File:** `pipeline/05_risk_degerlendirme.py`

**Purpose:** Combine PoF Ã— CoF into risk scores

**Logic:**

```python
# Merge PoF (12-month) with CoF
risk_df = pof_12m_df.merge(cof_df, on='cbs_id')

# Calculate risk score
risk_df['Risk_Score'] = risk_df['PoF_12M'] * risk_df['CoF']

# Risk classification
def classify_risk(row):
    pof = row['PoF_12M']
    cof = row['CoF']
    risk = row['Risk_Score']

    if risk >= 7.0:
        return 'KRÄ°TÄ°K'
    elif risk >= 5.0:
        return 'YÃœKSEK'
    elif risk >= 3.0:
        return 'ORTA'
    else:
        return 'DÃœÅÃœK'

risk_df['Risk_Sinifi'] = risk_df.apply(classify_risk, axis=1)
```

**Outputs:**
- `risk_skorlari_pof3.csv` - Risk scores
- `risk_skoru_ozet_ekipman_tipi.csv` - Summary by type

**Performance:** ~2 seconds

---

### Stage 05: Reporting

**File:** `pipeline/05_raporlama_ve_gorsellestirme.py`

**Purpose:** Generate deliverables

**Phases:**

#### 1. Action Lists

```python
# Critical equipment (0-30 days)
urgent = risk_df[risk_df['Risk_Sinifi'] == 'KRÄ°TÄ°K']

# CAPEX planning (high risk equipment)
capex = risk_df[risk_df['Risk_Sinifi'].isin(['KRÄ°TÄ°K', 'YÃœKSEK'])]

# Maintenance list (medium risk)
maintenance = risk_df[risk_df['Risk_Sinifi'] == 'ORTA']
```

#### 2. Visualizations

```python
import matplotlib.pyplot as plt

# Risk distribution
plt.figure(figsize=(10, 6))
risk_df['Risk_Sinifi'].value_counts().plot(kind='bar')
plt.title('Risk SÄ±nÄ±fÄ± DaÄŸÄ±lÄ±mÄ±')
plt.savefig('gorseller/risk_distribution.png')

# Equipment type distribution
# Fault trends
# PoF by horizon
# etc.
```

#### 3. Excel Report

```python
import pandas as pd

with pd.ExcelWriter('data/sonuclar/PoF_Analysis.xlsx') as writer:
    summary_df.to_excel(writer, sheet_name='Ã–zet', index=False)
    urgent_df.to_excel(writer, sheet_name='Acil MÃ¼dahale', index=False)
    capex_df.to_excel(writer, sheet_name='CAPEX PlanÄ±', index=False)
    # etc.
```

#### 4. PowerPoint (Optional)

```python
try:
    from pptx import Presentation

    prs = Presentation()
    slide = prs.slides.add_slide(prs.slide_layouts[0])
    title = slide.shapes.title
    title.text = "PoF3 Analiz SonuÃ§larÄ±"
    # Add charts, tables, etc.

    prs.save('data/sonuclar/PoF_Dashboard.pptx')
except ImportError:
    logger.warning("python-pptx not installed, skipping PowerPoint generation")
```

**Outputs:**
- CSV action lists
- PNG visualizations
- Excel report
- PowerPoint presentation (if library installed)

**Performance:** ~10 seconds

---

## ğŸ“Š Output Files

### CSV Files (data/sonuclar/)

| File | Rows | Columns | Description |
|------|------|---------|-------------|
| `risk_skorlari_pof3.csv` | All equipment | cbs_id, PoF_12M, CoF, Risk_Score, Risk_Sinifi | Main decision file |
| `risk_equipment_master.csv` | All equipment | Full metadata + risk | Comprehensive view |
| `chronic_equipment_summary.csv` | All equipment | chronic flags, Poisson p-value | IEEE 1366 analysis |
| `ensemble_pof_final.csv` | All equipment | ML ensemble predictions | Advanced PoF |
| `shap_feature_importance.csv` | Top features | feature, abs_importance | SHAP explainability |
| `rsf_feature_importance.csv` | All features | feature, importance | RSF rankings |
| `temporal_cv_scores.csv` | 3 folds | metric, fold, score | Model validation |

### Visualizations (gorseller/)

| File | Type | Description |
|------|------|-------------|
| `chronic_distribution.png` | Bar chart | Chronic equipment count |
| `equipment_distribution.png` | Pie chart | Equipment type breakdown |
| `fault_trends.png` | Line chart | Monthly fault trends |
| `feature_importance.png` | Horizontal bar | Top 15 features (SHAP) |
| `pof_by_horizon.png` | Box plot | PoF distribution by horizon |
| `survival_curves_by_class.png` | Kaplan-Meier | Survival by equipment type |
| `cox_coefficients.png` | Forest plot | Cox hazard ratios |

---

## ğŸ§ª Testing

### Current Status: âš ï¸ NO TESTS

**Critical Gap:** `/tests` directory is empty.

### Recommended Test Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # Pytest fixtures
â”œâ”€â”€ test_data_processing.py        # Stage 01 tests
â”œâ”€â”€ test_feature_engineering.py    # Stage 02 tests
â”œâ”€â”€ test_survival_models.py        # Stage 03 tests
â”œâ”€â”€ test_chronic_detection.py      # Stage 04 tests
â”œâ”€â”€ test_risk_scoring.py           # Stage 04b tests
â”œâ”€â”€ test_utilities.py              # Utils tests
â”œâ”€â”€ test_integration.py            # End-to-end tests
â””â”€â”€ fixtures/
    â”œâ”€â”€ sample_fault_data.csv
    â”œâ”€â”€ sample_healthy_data.csv
    â””â”€â”€ expected_outputs.csv
```

### Example Unit Test

```python
# tests/test_data_processing.py

import pytest
from pipeline.01_veri_isleme import load_and_validate_fault_data

def test_fault_data_loading(sample_fault_file):
    """Test fault data loading with valid input."""
    df = load_and_validate_fault_data(sample_fault_file)

    assert len(df) > 0
    assert 'cbs_id' in df.columns
    assert 'Ariza_Baslangic_Zamani' in df.columns
    assert df['cbs_id'].notna().all()

def test_outlier_detection():
    """Test MAD-based outlier detection."""
    # Implement test
    pass

def test_turkish_date_parsing():
    """Test multi-format date parsing."""
    from utils.date_parser import parse_mixed_dates

    dates = [
        "1.2.2021 16:59",
        "07-01-2024 21:17:45",
        "2021-02-01 14:30:00"
    ]
    parsed = parse_mixed_dates(dates)

    assert len(parsed) == 3
    assert all(pd.notna(parsed))
```

### Running Tests

```bash
# Install pytest
pip install pytest pytest-cov

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=pipeline --cov=utils --cov-report=html

# Run specific test file
pytest tests/test_data_processing.py -v
```

### Integration Test

```bash
# Full pipeline test with sample data
pytest tests/test_integration.py -v -s
```

---

## ğŸš¢ Deployment

### Production Deployment Options

#### Option 1: Scheduled Batch Job

```bash
# Cron job (Linux)
# Run pipeline monthly on 1st day at 2 AM
0 2 1 * * cd /opt/PoF3 && /opt/PoF3/.venv/bin/python orchestrator/run_pipeline.py >> /var/log/pof3.log 2>&1

# Windows Task Scheduler
# Create scheduled task running:
C:\PoF3\.venv\Scripts\python.exe C:\PoF3\orchestrator\run_pipeline.py
```

#### Option 2: Docker Container

**Dockerfile:**

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Run pipeline
CMD ["python", "orchestrator/run_pipeline.py"]
```

**Build & Run:**

```bash
# Build image
docker build -t pof3:latest .

# Run container
docker run -v /path/to/data:/app/data pof3:latest
```

#### Option 3: FastAPI Wrapper (Future)

```python
# api/main.py (not yet implemented)

from fastapi import FastAPI, UploadFile
from pipeline.03_sagkalim_modelleri import predict_pof

app = FastAPI()

@app.post("/predict")
async def predict_equipment_pof(equipment_data: dict):
    """Predict PoF for single equipment."""
    pof_12m = predict_pof(equipment_data, horizon=365)
    return {"cbs_id": equipment_data['cbs_id'], "pof_12m": pof_12m}

@app.post("/batch-predict")
async def batch_predict(file: UploadFile):
    """Batch prediction from CSV."""
    # Implement batch prediction
    pass
```

**Run API:**

```bash
pip install fastapi uvicorn
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

### Environment Variables

```bash
# .env file (not tracked in git)
DATA_DIR=/opt/pof3/data
LOG_LEVEL=INFO
ENABLE_SHAP=true
ENABLE_PPTX=false
```

**Load in Python:**

```python
from dotenv import load_dotenv
import os

load_dotenv()
log_level = os.getenv('LOG_LEVEL', 'INFO')
```

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

#### Issue 1: SHAP Not Installed

**Error:**
```
[WARN] SHAP computation failed: No module named 'shap'
```

**Solution:**
```bash
pip install shap>=0.43.0
```

**Impact:** Pipeline continues, SHAP importance skipped.

---

#### Issue 2: Matplotlib Backend Error

**Error:**
```
RuntimeError: main thread is not in main loop
```

**Solution:**
```python
# Add to top of visualization scripts
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
```

---

#### Issue 3: Memory Error (Large Datasets)

**Error:**
```
MemoryError: Unable to allocate array
```

**Solution:**
```python
# config/config.py
RSF_N_ESTIMATORS = 50  # Reduce from 100
MAX_SHAP_SAMPLES = 500  # Reduce from 1000

# Or increase system RAM
```

---

#### Issue 4: Date Parsing Failures

**Error:**
```
ValueError: time data '...' does not match format
```

**Solution:**
Check `utils/date_parser.py` supports all date formats in your data.

```python
# Add new format to parse_mixed_dates()
formats = [
    '%d.%m.%Y %H:%M',
    '%d-%m-%Y %H:%M:%S',
    '%Y-%m-%d %H:%M:%S',
    '%d/%m/%Y %H:%M',
    '%Y%m%d %H%M%S'  # Add new format
]
```

---

#### Issue 5: Missing Input Files

**Error:**
```
FileNotFoundError: data/girdiler/ariza_final.xlsx
```

**Solution:**
Ensure input files exist:
```bash
ls -la data/girdiler/
# Should show: ariza_final.xlsx, saglam_final.xlsx
```

---

#### Issue 6: Temporal CV Fails

**Error:**
```
ValueError: Not enough data for 3-fold CV
```

**Solution:**
Reduce CV folds in `utils/ml_advanced.py`:
```python
def temporal_cross_validation(..., n_splits=2):  # Change from 3
```

---

### Debugging Steps

1. **Check Logs:**
   ```bash
   tail -f loglar/pipeline_master_*.log
   ```

2. **Validate Input Data:**
   ```python
   python -c "from config.config import validate_config; validate_config()"
   ```

3. **Run Individual Stages:**
   ```bash
   python pipeline/01_veri_isleme.py  # Isolate issue
   ```

4. **Enable Debug Mode:**
   ```python
   # In logger.py, change level
   logging.basicConfig(level=logging.DEBUG)
   ```

---

## ğŸ’» Development Guidelines

### Code Style

**Follow PEP 8:**

```bash
# Install linters
pip install black isort flake8 mypy

# Format code
black pipeline/ utils/
isort pipeline/ utils/

# Check style
flake8 pipeline/ utils/

# Type check
mypy pipeline/ utils/
```

### Git Workflow

```bash
# Create feature branch
git checkout -b feature/add-new-model

# Make changes
# ...

# Commit with descriptive messages
git add .
git commit -m "feat: add Weibull survival model to Stage 03"

# Push and create PR
git push origin feature/add-new-model
```

### Adding New Features

**Example: Add Weibull Model to Stage 03**

1. **Update config/config.py:**
   ```python
   USE_WEIBULL = True
   ```

2. **Add function to pipeline/03_sagkalim_modelleri.py:**
   ```python
   from lifelines import WeibullAFTFitter

   def fit_weibull_model(df):
       weibull = WeibullAFTFitter()
       weibull.fit(df, duration_col='duration_days', event_col='event')
       return weibull
   ```

3. **Add output path to config:**
   ```python
   WEIBULL_POF_12M = OUTPUT_DIR / "weibull_sagkalim_12ay_ariza_olasiligi.csv"
   ```

4. **Update tests:**
   ```python
   def test_weibull_model():
       # Test implementation
       pass
   ```

5. **Update documentation:**
   - Add to BENIOKU_TEKNIK.md
   - Update IMPLEMENTATION_SUMMARY.md

---

## ğŸ“š API Reference

### Utility Functions

#### utils/logger.py

```python
def setup_logger(name: str, log_file: str, level: int = logging.INFO) -> logging.Logger:
    """
    Setup logger with file and console handlers.

    Args:
        name: Logger name
        log_file: Path to log file
        level: Logging level (INFO, DEBUG, etc.)

    Returns:
        Configured logger instance
    """
```

#### utils/ml_advanced.py

```python
def temporal_cross_validation(
    X: pd.DataFrame,
    y: np.ndarray,
    model_fn: Callable,
    n_splits: int = 3,
    logger: logging.Logger = None
) -> dict:
    """
    Perform time-series cross-validation.

    Args:
        X: Feature matrix
        y: Target variable
        model_fn: Function returning model instance
        n_splits: Number of CV folds
        logger: Logger instance

    Returns:
        Dict with AUC and AP scores per fold
    """

def compute_shap_importance(
    model,
    X: pd.DataFrame,
    max_samples: int = 1000,
    logger: logging.Logger = None
) -> pd.DataFrame:
    """
    Compute SHAP feature importance.

    Args:
        model: Trained XGBoost/CatBoost model
        X: Feature matrix
        max_samples: Max samples for SHAP (performance)
        logger: Logger instance

    Returns:
        DataFrame with feature importance
    """
```

#### utils/date_parser.py

```python
def parse_mixed_dates(date_series: pd.Series) -> pd.Series:
    """
    Parse Turkish dates in mixed formats.

    Supported formats:
    - 1.2.2021 16:59
    - 07-01-2024 21:17:45
    - 2021-02-01 14:30:00
    - 01/02/2021 09:30

    Args:
        date_series: Pandas series with date strings

    Returns:
        Pandas series with datetime objects
    """
```

---

## âš¡ Performance Optimization

### Profiling

```python
# Add to top of script
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()

# Your code here

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 functions
```

### Optimization Tips

1. **Reduce SHAP Samples:**
   ```python
   # utils/ml_advanced.py
   MAX_SHAP_SAMPLES = 500  # Instead of 1000
   ```

2. **Use Parallel Processing:**
   ```python
   from joblib import Parallel, delayed

   results = Parallel(n_jobs=-1)(
       delayed(process_equipment)(eq) for eq in equipment_list
   )
   ```

3. **Cache Intermediate Results:**
   ```python
   import joblib

   # Save
   joblib.dump(model, 'models/rsf_12m.pkl')

   # Load
   model = joblib.load('models/rsf_12m.pkl')
   ```

4. **Use Categorical Dtypes:**
   ```python
   df['Ekipman_Tipi'] = df['Ekipman_Tipi'].astype('category')
   ```

### Performance Benchmarks

| Dataset Size | Step 03 Time | Total Pipeline | RAM Usage |
|--------------|--------------|----------------|-----------|
| 1K equipment | ~30s | ~60s | <1GB |
| 6K equipment | ~95s | ~150s | 2GB |
| 50K equipment | ~8min | ~12min | 6GB |

---

## ğŸ“ Support & Contribution

### Reporting Issues

Create GitHub issue with:
1. Error message (full stack trace)
2. Steps to reproduce
3. Input data sample (anonymized)
4. Python version, OS
5. Logs from `loglar/`

### Contributing

1. Fork repository
2. Create feature branch
3. Add tests for new features
4. Update documentation
5. Submit pull request

### Code Review Checklist

- [ ] Code follows PEP 8
- [ ] Tests added and passing
- [ ] Documentation updated
- [ ] Changelog updated
- [ ] No hardcoded paths/credentials
- [ ] Error handling implemented
- [ ] Logging added

---

## ğŸ“„ License

**Enterprise License**
Â© 2025 PoF3 Project. All rights reserved.

---

**Last Updated:** December 2025
**Version:** 3.1
**Maintainer:** Technical Team
